{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import string, os\n",
    "import tensorflow as tf\n",
    "\n",
    "# keras module for building LSTM\n",
    "from keras.utils import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dropout, LSTM, Dense, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('song_lyrics_generator_Not_One.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_sequences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m max_sequence_len \u001b[39m=\u001b[39m \u001b[39m968\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[39m# Gets the length of the longest list in the\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39m# array of lists\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m input_sequences \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(pad_sequences(input_sequences, maxlen\u001b[39m=\u001b[39mmax_sequence_len, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpre\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m     12\u001b[0m \u001b[39m# Dividing the data into X, y -----> the training set, and the labels to be predicted\u001b[39;00m\n\u001b[0;32m     13\u001b[0m X, labels \u001b[39m=\u001b[39m input_sequences[:,:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],input_sequences[:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'input_sequences' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "total_words = 4787\n",
    "# We add one to the index, as index starts from zero\n",
    "\n",
    "\n",
    "# Pre padding\n",
    "max_sequence_len = 968\n",
    "\n",
    "# Gets the length of the longest list in the\n",
    "# array of lists\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Dividing the data into X, y -----> the training set, and the labels to be predicted\n",
    "X, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "# Takes all elemnts in each row, except the last element, and places them in X\n",
    "# while labels takes the last element (the element which we should predict)\n",
    "\n",
    "y = tf.keras.utils.to_categorical(labels, num_classes=total_words) # One hot encoding\n",
    "# number of classes is now equal to the number of unique words in the song lyrics\n",
    "# creating model\n",
    "model = Sequential()\n",
    "#model = Sequential()\n",
    "model.add(Embedding(total_words, 40, input_length=max_sequence_len-1))\n",
    "# dimension of input: total_words, the number of unique words we have\n",
    "# 40: the desired dimension of the output\n",
    "# input_length: the sequence length is all the words except the last one (the one\n",
    "# we will predict)\n",
    "model.add(Bidirectional(LSTM(250))) # 250 is the average number of words in a song\n",
    "# So our cycle is the average length of a song\n",
    "# We used LSTM instead of simple RNN as simple RNN faces a vanishing gradient\n",
    "# problem, also, we need to remember the previous words, to predict the next words.\n",
    "\n",
    "model.add(Dropout(0.1)) # To overcome overfitting\n",
    "model.add(Dense(total_words, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_this_song(input_text, next_words):\n",
    "    for _ in range(next_words):\n",
    "        # for _ in... this is like a place holder, which upholds the syntax.\n",
    "        # We use it when we don't want to use the variable, so we leave it empty.\n",
    "\n",
    "        # Doing the same things to the input as we did when training the model\n",
    "        token_list = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        #predicted = model.predict_classes(token_list, verbose=0)\n",
    "        predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                # Gets the word corresponding the the value predicted\n",
    "                # [Converting from numeric to string again]\n",
    "                output_word = word\n",
    "                break\n",
    "        input_text += \" \" + output_word\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I miss the nights with you messy ambition messy spotlights station station juliet station station juliet station station station everybody station station g5 made wycd station bleachers station bleachers station station bleachers station station â€œit's wycd station bleachers station station bleachers station station everybody station station g5 what'd station bleachers station station bleachers station station bleachers\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_this_song(\"I miss the nights with you\", 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lyrics_generator",
   "language": "python",
   "name": "lyrics_generator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
