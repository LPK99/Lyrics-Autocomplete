{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import string, os\n",
    "import tensorflow as tf\n",
    "\n",
    "# keras module for building LSTM\n",
    "from keras.utils import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Dropout, LSTM, Dense, Bidirectional\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArtistName is not present in the DataFrame\n"
     ]
    }
   ],
   "source": [
    "# csv file\n",
    "df = pd.read_csv('lyrics-data.csv')\n",
    "# Assuming you have a DataFrame called df\n",
    "# Check if \"ArtistName\" is in the \"artist\" column\n",
    "is_artist_present = df['ALink'].isin(['/beatles/'])\n",
    "\n",
    "# Check if \"ArtistName\" is present in the DataFrame\n",
    "if is_artist_present.any():\n",
    "    print(\"ArtistName is present in the DataFrame\")\n",
    "else:\n",
    "    print(\"ArtistName is not present in the DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lyric</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17367</th>\n",
       "      <td>Gravity is working against me\\nAnd gravity wan...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17368</th>\n",
       "      <td>It's not a silly little moment\\nIt's not the s...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17369</th>\n",
       "      <td>Your love is bright as ever\\nEven in the shado...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17370</th>\n",
       "      <td>Lightning strikes\\nInside my chest to keep me ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17371</th>\n",
       "      <td>I'm the boy in your other phone\\nLighting up i...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17513</th>\n",
       "      <td>River's strong, you can't swim inside it\\nWe c...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17514</th>\n",
       "      <td>Only a nascent trying to harness huge fire\\nOu...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17515</th>\n",
       "      <td>It's been so long\\nsince I've seen you.\\nAnd I...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17516</th>\n",
       "      <td>A great Big Bang and dinosaurs\\nFiery raining ...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17517</th>\n",
       "      <td>No, I've not seen you this way before\\nStandin...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Lyric language\n",
       "17367  Gravity is working against me\\nAnd gravity wan...       en\n",
       "17368  It's not a silly little moment\\nIt's not the s...       en\n",
       "17369  Your love is bright as ever\\nEven in the shado...       en\n",
       "17370  Lightning strikes\\nInside my chest to keep me ...       en\n",
       "17371  I'm the boy in your other phone\\nLighting up i...       en\n",
       "...                                                  ...      ...\n",
       "17513  River's strong, you can't swim inside it\\nWe c...       en\n",
       "17514  Only a nascent trying to harness huge fire\\nOu...       en\n",
       "17515  It's been so long\\nsince I've seen you.\\nAnd I...       en\n",
       "17516  A great Big Bang and dinosaurs\\nFiery raining ...       en\n",
       "17517  No, I've not seen you this way before\\nStandin...       en\n",
       "\n",
       "[150 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df[df['language'] =='en']\n",
    "df = df[df['ALink'] == '/john-mayer/']\n",
    "df.drop(['ALink','SName','SLink'],axis=1,inplace=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2546\n",
      "565\n",
      "[[  0   0   0 ...   0   0 574]\n",
      " [  0   0   0 ...   0 574  15]\n",
      " [  0   0   0 ... 574  15 389]\n",
      " ...\n",
      " [  0   0   0 ...  42 293 135]\n",
      " [  0   0   0 ... 293 135 731]\n",
      " [  0   0   0 ... 135 731   3]] [ 15 389 654 ... 731   3  43]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "# Used to preprocess the text, by removing the comma and\n",
    "# other punctuations for example. We also removed numbers\n",
    "# It also splits the sentences to words, and turns all words\n",
    "# into lower case\n",
    "\n",
    "tokenizer.fit_on_texts(df['Lyric'].astype(str).str.lower())\n",
    "# Applying the tokenizer\n",
    "\n",
    "total_words = len(tokenizer.word_index)+1\n",
    "print(total_words)\n",
    "# We add one to the index, as index starts from zero\n",
    "\n",
    "tokenized_sentences = tokenizer.texts_to_sequences(df['Lyric'].astype(str))\n",
    "# Turns the words into integer type by classifying them according to word dictionary.\n",
    "#tokenized_sentences[0]\n",
    "# Checking the first element, to understand what happened\n",
    "# Slash sequences into n gram sequence\n",
    "input_sequences = list()\n",
    "for i in tokenized_sentences:\n",
    "    for t in range(1, len(i)):\n",
    "        n_gram_sequence = i[:t+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "# Having reached this point, input_sequences contains array of lists of words, starting from\n",
    "# a list containg 2 words, then we append a third word to them, then another word, and so on.\n",
    "# This causes a problem, as the length of the lists are not equal, so we pad them by adding zeros\n",
    "# at the beginnig, till they are all of the same length (the maximum length)\n",
    "\n",
    "# Pre padding\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "print (max_sequence_len)\n",
    "# Gets the length of the longest list in the\n",
    "# array of lists\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "# Dividing the data into X, y -----> the training set, and the labels to be predicted\n",
    "X, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "print(X, labels)\n",
    "# Takes all elemnts in each row, except the last element, and places them in X\n",
    "# while labels takes the last element (the element which we should predict)\n",
    "\n",
    "y = tf.keras.utils.to_categorical(labels, num_classes=total_words) # One hot encoding\n",
    "print(y)\n",
    "# number of classes is now equal to the number of unique words in the song lyrics\n",
    "# creating model\n",
    "model = Sequential()\n",
    "#model = Sequential()\n",
    "model.add(Embedding(total_words, 40, input_length=max_sequence_len-1))\n",
    "# dimension of input: total_words, the number of unique words we have\n",
    "# 40: the desired dimension of the output\n",
    "# input_length: the sequence length is all the words except the last one (the one\n",
    "# we will predict)\n",
    "model.add(Bidirectional(LSTM(250))) # 250 is the average number of words in a song\n",
    "# So our cycle is the average length of a song\n",
    "# We used LSTM instead of simple RNN as simple RNN faces a vanishing gradient\n",
    "# problem, also, we need to remember the previous words, to predict the next words.\n",
    "\n",
    "model.add(Dropout(0.1)) # To overcome overfitting\n",
    "model.add(Dense(total_words, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_this_song(input_text, next_words, model):\n",
    "    for _ in range(next_words):\n",
    "        # for _ in... this is like a place holder, which upholds the syntax.\n",
    "        # We use it when we don't want to use the variable, so we leave it empty.\n",
    "\n",
    "        # Doing the same things to the input as we did when training the model\n",
    "        token_list = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        #predicted = model.predict_classes(token_list, verbose=0)\n",
    "        predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                # Gets the word corresponding the the value predicted\n",
    "                # [Converting from numeric to string again]\n",
    "                output_word = word\n",
    "                break\n",
    "        input_text += \" \" + output_word\n",
    "    return input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('song_lyrics_generator_john_mayer.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 535ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I miss the nights with you always done the same one i will be good to the level i tried the one but one is is the same mistake i used to make me if i ever get around to living i just just like you think i'm gonna be just from me and i want\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_this_song(\"I miss the nights with you\", 50, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lyrics_generator",
   "language": "python",
   "name": "lyrics_generator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
